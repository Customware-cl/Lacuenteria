√âpica: WIZARD - [2] DISE√ëO DE HISTORIA
Categor√≠a: bug/*
Notas para devs: Los modelos serie O usan tokens para razonamiento interno antes de generar la respuesta

Archivos afectados:
- /home/customware/lacuenteria/Lacuenteria/supabase/functions/generate-story/index.ts

üß† Contexto:
Los modelos de la serie O (o1, o3, o4) devuelven respuestas vac√≠as cuando se les asigna el mismo l√≠mite de tokens que otros modelos. Esto ocurre porque dividen los tokens entre "reasoning_tokens" (razonamiento interno) y tokens de respuesta. Con 1500 tokens, el modelo o4-mini us√≥ todos los tokens en razonamiento y no gener√≥ contenido, resultando en finish_reason: "length".

üìê Objetivo:
Aumentar el l√≠mite de tokens para modelos serie O para garantizar que tengan suficientes tokens tanto para razonamiento como para generar la respuesta completa del cuento.

‚úÖ CRITERIOS DE √âXITO (lo que s√≠ debe ocurrir):
- Los modelos serie O reciben 4000 tokens de l√≠mite
- Los dem√°s modelos mantienen 1500 tokens
- Las respuestas de modelos serie O contienen el JSON completo del cuento
- No se cortan las respuestas por falta de tokens
- El finish_reason debe ser "stop" no "length"

‚ùå CRITERIOS DE FALLA (lo que no debe ocurrir):
- No debe devolver contenido vac√≠o
- No debe fallar con error "La respuesta de OpenAI no contiene contenido"
- No debe aumentar innecesariamente los tokens para modelos normales
- No debe afectar el costo para modelos que no lo necesitan

üß™ QA / Casos de prueba esperados:
- Generar historia con o4-mini ‚Üí debe devolver JSON completo sin cortes
- Verificar en logs ‚Üí completion_tokens < 4000 y reasoning_tokens > 0
- Generar historia con gpt-4o ‚Üí debe seguir usando 1500 tokens m√°ximo
- Verificar finish_reason ‚Üí debe ser "stop" para ambos tipos de modelos

EXTRAS:
- Los modelos serie O son √∫nicos en su uso de reasoning_tokens
- El l√≠mite de 4000 tokens es conservador pero garantiza respuestas completas
- Considerar ajustar seg√∫n m√©tricas reales de uso en producci√≥n
- Los reasoning_tokens aparecen en usage.completion_tokens_details
