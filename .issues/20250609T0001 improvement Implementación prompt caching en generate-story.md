√âpica: WIZARD - [2] DISE√ëO DE HISTORIA
Categor√≠a: improvement/*
Notas para devs: Optimizaci√≥n de la funci√≥n generate-story con prompt caching

Archivos afectados:
- /home/customware/lacuenteria/Lacuenteria/supabase/functions/generate-story/index.ts

üß† Contexto:
Se implement√≥ prompt caching en la funci√≥n generate-story para optimizar la latencia y reducir costos en las llamadas a OpenAI. El prompt de generaci√≥n de cuentos es largo (>1024 tokens) y tiene una estructura que permite separar contenido est√°tico de variable.

üìê Objetivo:
- Reducir latencia en la generaci√≥n de historias mediante prompt caching
- Disminuir costos aprovechando el descuento del 50% en tokens cacheados
- Mantener la funcionalidad existente mientras se optimiza el rendimiento

‚úÖ CRITERIOS DE √âXITO (lo que s√≠ debe ocurrir):
- El prompt se estructura con contenido est√°tico primero y variable al final
- Se incluye el par√°metro `user` con el userId en las llamadas a OpenAI
- La parte est√°tica del prompt (instrucciones) se separa de la variable (personajes/tema)
- El prompt final mantiene el formato esperado por el modelo
- La generaci√≥n de historias contin√∫a funcionando correctamente

‚ùå CRITERIOS DE FALLA (lo que no debe ocurrir):
- No se rompe el formato JSON esperado en la respuesta
- No se pierden las instrucciones del prompt original
- No se alteran los personajes o temas proporcionados
- No se generan errores en la llamada a OpenAI

üß™ QA / Casos de prueba esperados:
- Generar historia con 1 personaje ‚Üí debe funcionar y usar caching
- Generar historia con m√∫ltiples personajes ‚Üí debe mantener formato correcto
- Generar m√∫ltiples historias con mismo usuario ‚Üí debe aprovechar cache
- Verificar en logs que el prompt se estructura correctamente

EXTRAS:
- El prompt caching se activa autom√°ticamente para prompts > 1024 tokens
- OpenAI cachea las partes est√°ticas del prompt por hasta 1 hora
- El descuento del 50% aplica a los tokens cacheados
- La estructura √≥ptima es: instrucciones est√°ticas + datos variables al final
